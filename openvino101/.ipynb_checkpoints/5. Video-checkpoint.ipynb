{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ca2fe7e-41e3-4f7c-aa76-3bb4cb74a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import openvino as ov\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce897a-c1a4-497e-ba77-f2840e4fa9b2",
   "metadata": {},
   "source": [
    "# 웹캠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b36a8d37-7f89-47c8-908e-c4927c6e9ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b22a4e75-8afb-4dda-830c-185fe3732a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d176cb8b-4b2b-4f49-8f3a-1c1ef64db172",
   "metadata": {},
   "source": [
    "# Face Detection with Webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55167ef-58fb-4466-b1e5-6b49ccab6916",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dbc2c37-765a-42c7-b407-40a5bd124dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: [1,3,384,672]\n",
      "Output shape: [1,1,200,7]\n",
      "Input shape: [1,3,64,64]\n",
      "Output shape: [1,5,1,1]\n",
      "Input shape: [1,3,62,62]\n",
      "Output shape: [1,2,1,1]\n"
     ]
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "# face model\n",
    "model_face = core.read_model(model='models/face-detection-adas-0001.xml')\n",
    "compiled_model_face = core.compile_model(model = model_face, device_name=\"CPU\")\n",
    "\n",
    "input_layer_face = compiled_model_face.input(0)\n",
    "output_layer_face = compiled_model_face.output(0)\n",
    "\n",
    "print(f\"Input shape: {input_layer_face.shape}\")\n",
    "print(f\"Output shape: {output_layer_face.shape}\")\n",
    "\n",
    "# emotion model\n",
    "model_emo = core.read_model(model='models/emotions-recognition-retail-0003.xml')\n",
    "compiled_model_emo = core.compile_model(model = model_emo, device_name=\"CPU\")\n",
    "\n",
    "input_layer_emo = compiled_model_emo.input(0)\n",
    "output_layer_emo = compiled_model_emo.output(0)\n",
    "\n",
    "print(f\"Input shape: {input_layer_emo.shape}\")\n",
    "print(f\"Output shape: {output_layer_emo.shape}\")\n",
    "\n",
    "# age, gender model\n",
    "model_ag = core.read_model(model='models/age-gender-recognition-retail-0013.xml')\n",
    "compiled_model_ag = core.compile_model(model = model_ag, device_name=\"CPU\")\n",
    "\n",
    "input_layer_ag = compiled_model_ag.input(0)\n",
    "output_layer_ag = compiled_model_ag.output(0)\n",
    "\n",
    "print(f\"Input shape: {input_layer_ag.shape}\")\n",
    "print(f\"Output shape: {output_layer_ag.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e127f0-ebe7-4e7f-86f7-f7badfdb12e0",
   "metadata": {},
   "source": [
    "### Pre-Process New Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44ae03cb-264a-4a3c-93a4-49acb32767e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, input_layer_face):\n",
    "    N, input_channels, input_height, input_width = input_layer_face.shape\n",
    "\n",
    "    resized_image = cv2.resize(image, (input_width, input_height))\n",
    "    transposed_image = resized_image.transpose(2, 0, 1)\n",
    "    input_image = np.expand_dims(transposed_image, 0)\n",
    "\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d0ba1-794b-45ac-bc49-7438938f3557",
   "metadata": {},
   "source": [
    "### Postprocess the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57f570-b76f-4c01-b038-be6807f34035",
   "metadata": {},
   "source": [
    "#### Find the Face Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25a8f476-a46b-4d2e-9b54-3a7e8a4058f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faceboxes(image, results, confidence_threshold):\n",
    "    results = results.squeeze()\n",
    "\n",
    "    scores = results[:, 2]\n",
    "    boxes = results[:, -4:]\n",
    "\n",
    "    face_boxes = boxes[scores >= confidence_threshold]\n",
    "    scores = scores[scores >= confidence_threshold]\n",
    "\n",
    "    image_h, image_w, image_channels = image.shape\n",
    "    face_boxes = face_boxes * np.array([image_w, image_h, image_w, image_h])\n",
    "    face_boxes = face_boxes.astype(np.int64)\n",
    "\n",
    "    return face_boxes, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d7c498-98f7-4886-9379-71117c6de844",
   "metadata": {},
   "source": [
    "### Draw the Emotion/Age/Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aadc054-ff41-403b-81ea-a2253b026557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_age_gender_emotion(face_boxes, frame):\n",
    "    EMOTION_NAMES = ['neutral', 'happy', 'sad', 'surprise', 'anger']\n",
    "    \n",
    "    show_frame = frame.copy()\n",
    "    \n",
    "    for i in range(len(face_boxes)):\n",
    "        \n",
    "        xmin, ymin, xmax, ymax = face_boxes[i]\n",
    "        face = frame[ymin:ymax, xmin:xmax]\n",
    "\n",
    "        # --- emotion ---\n",
    "        input_frame = preprocess(frame, input_layer_emo)\n",
    "        results_emo = compiled_model_emo([input_frame])[output_layer_emo]\n",
    "    \n",
    "        results_emo = results_emo.squeeze()\n",
    "        index = np.argmax(results_emo)\n",
    "        # ----------------\n",
    "        \n",
    "        # --- age and gender ---\n",
    "        input_frame_ag = preprocess(frame, input_layer_ag)\n",
    "        results_ag = compiled_model_ag([input_frame_ag])\n",
    "        age, gender = results_ag[1], results_ag[0]\n",
    "        age = np.squeeze(age)\n",
    "        age = int(age*100)\n",
    "\n",
    "        gender = np.squeeze(gender)\n",
    "        \n",
    "        if (gender[0]>=0.65):\n",
    "            gender = \"female\"\n",
    "            box_color = (200, 200, 0)\n",
    "            \n",
    "        elif (gender[1]>=0.55):\n",
    "            gender = \"male\"\n",
    "            box_color = (0, 200, 200)\n",
    "        \n",
    "        else:\n",
    "            gender = \"Unknown\"\n",
    "            box_color = (0, 200, 200)\n",
    "            \n",
    "        # -----------------------\n",
    "\n",
    "        fontScale = frame.shape[1]/750\n",
    "        \n",
    "        text = gender + ' ' + str(age) + ' ' + EMOTION_NAMES[index]\n",
    "        cv2.putText(show_frame, text, (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (0, 200, 0), 10)\n",
    "        cv2.rectangle(img=show_frame, pt1=(xmin, ymin), pt2=(xmax, ymax), color=box_color, thickness=2)\n",
    "\n",
    "    return show_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de3f0f-b01b-4fce-b438-a7c43b7b77d3",
   "metadata": {},
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3b8baa1-0b3e-4409-92de-24cfa6fe3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    camera = cv2.VideoCapture(source)\n",
    "\n",
    "    while (True):\n",
    "        ret, frame = camera.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        input_image = preprocess(frame, input_layer_face)\n",
    "        results = compiled_model_face([input_image])[output_layer_face]\n",
    "        \n",
    "        face_boxes, scores = find_faceboxes(frame, results, confidence_threshold)\n",
    "        show_image = draw_age_gender_emotion(face_boxes, frame)\n",
    "\n",
    "        cv2.imshow(\"Webcam\", show_image)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "            break\n",
    "\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f52c0eed-90a7-4f57-a721-6413a036ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = .95\n",
    "source = \"data/video.mp4\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c0a99-fdfe-4356-ac8d-9ef6bedde001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
